{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "END_Capstone.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1QwBWtjUOFvSQrnX+eZbq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paishowstopper/TSAI/blob/main/Capstone/END_Capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmiaOTYgWce3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import json\n",
        "import re\n",
        "\n",
        "from tokenize import tokenize\n",
        "from io import BytesIO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6ph29jsS8-0"
      },
      "source": [
        "def format_solution(solution):\n",
        "    tmp_solution = []\n",
        "    for sentence in solution:\n",
        "        if sentence == '\\n' or re.match(r' +\\n|\\t+\\n', sentence):\n",
        "            continue\n",
        "        else:\n",
        "            trailing_spaces = re.match(r'(^.*?)[ ]+\\n', sentence)\n",
        "            if trailing_spaces:\n",
        "                sentence = re.sub(r'(^.*?)[ ]+\\n', r'\\1\\n', sentence)\n",
        "            tmp_solution.append(sentence)\n",
        "    solution = tmp_solution\n",
        "\n",
        "    def find_indent_value(starting_spaces_, indent_scheme_):\n",
        "        sum_of_indents = 0\n",
        "        for i, j in enumerate(indent_scheme_):\n",
        "            sum_of_indents += j\n",
        "            if sum_of_indents == starting_spaces_:\n",
        "                break\n",
        "        total_indents_ = i\n",
        "        if total_indents_ < 0:\n",
        "            raise Exception\n",
        "\n",
        "        return total_indents_\n",
        "\n",
        "    pruned_solution = []\n",
        "    indent_scheme = []\n",
        "    check_indentation_flag = True\n",
        "    for sentence_number,sentence in enumerate(solution):\n",
        "        starting_spaces = len(re.match(r'^([ ]*).*?', sentence)[1])\n",
        "        if check_indentation_flag:\n",
        "            possible_indent = starting_spaces - sum(indent_scheme)\n",
        "            if possible_indent > 0 or sentence_number == 0: # checking sentence number to get the first indentation which cud be 0\n",
        "                indent_scheme.append(possible_indent)\n",
        "            check_indentation_flag = False\n",
        "        if re.match(r'.*:\\n', sentence):\n",
        "            check_indentation_flag = True\n",
        "\n",
        "        total_indents = find_indent_value(starting_spaces, indent_scheme)\n",
        "        indent = total_indents * '\\t'\n",
        "        pruned_solution.append(re.sub(r'([ ]*)(.*?)\\n', indent + r'\\2\\n', sentence))\n",
        "\n",
        "    return pruned_solution\n",
        "\n",
        "def getData(path):\n",
        "    question = re.compile(r'^#[ ]?[0-9]*[.]?[ ]?(.*)')\n",
        "    unique_questions_with_different_solution = {}  # used to check for unique answers\n",
        "    question_answer_pair = {}  # used to save solutions for unique questions\n",
        "    f = open(path, \"r\")\n",
        "\n",
        "    solution = []\n",
        "    for i, line in enumerate(f):\n",
        "        match_object = question.match(line)\n",
        "        if match_object:\n",
        "            if len(solution) == 0:\n",
        "                prev_match = match_object[1]\n",
        "                continue\n",
        "            else:\n",
        "                solution_combined = \" \".join(solution).replace('\\n', \"\").replace('\\t', \"\").replace(\" \", \"\")\n",
        "                if prev_match not in unique_questions_with_different_solution:\n",
        "                    unique_questions_with_different_solution[prev_match] = [solution_combined]\n",
        "                    question_answer_pair[prev_match] = [solution]\n",
        "                else:\n",
        "                    flag = 0\n",
        "                    for j in unique_questions_with_different_solution[prev_match]:\n",
        "                        if j != solution_combined:\n",
        "                            flag = 1\n",
        "                    if flag:\n",
        "                        unique_questions_with_different_solution[prev_match].append(solution_combined)\n",
        "                        question_answer_pair[prev_match].append(solution)\n",
        "            solution = []\n",
        "            prev_match = match_object[1]\n",
        "        else:\n",
        "            solution.append(line)\n",
        "\n",
        "    questions_list = []\n",
        "    answers_list = []\n",
        "    for i in question_answer_pair:\n",
        "        for j in question_answer_pair[i]:\n",
        "            questions_list.append(i)\n",
        "            formatted_solution = format_solution(j)\n",
        "            answers_list.append(\"\".join(formatted_solution))\n",
        "\n",
        "    return questions_list,answers_list\n",
        "\n",
        "\n",
        "def getTokenizer(python_code):\n",
        "    tokens = []\n",
        "    try:\n",
        "        a = list(tokenize(BytesIO(python_code.encode('utf-8')).readline))\n",
        "        for i__ in a[1:-1]:\n",
        "            if i__.exact_type == 3:\n",
        "                if re.match(r'^f\"', i__[1]):\n",
        "                    string_tokens = ['f\"'] + [k__ for k__ in i__[1][2:]]\n",
        "                elif re.match(r\"^f'\", i__[1]):\n",
        "                    string_tokens = [\"f'\"] + [k__ for k__ in i__[1][2:]]\n",
        "                else:\n",
        "                    string_tokens = [k__ for k__ in i__[1]]\n",
        "                tokens = tokens + string_tokens\n",
        "            elif i__.exact_type == 6:   # removing dedent tokens\n",
        "                continue\n",
        "            else:\n",
        "                tokens.append(i__[1])\n",
        "    except Exception:\n",
        "        print(\"Error in tokenization\")\n",
        "\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a1SGvMF2k-l",
        "outputId": "611e51bb-ea7a-495d-f139-6c12c64714dd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85PofHuo8vS-",
        "outputId": "78a7f805-9d4b-406c-d5e5-10efb9134e81"
      },
      "source": [
        "cd 'drive/MyDrive/capstone-data'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/capstone-data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NakaRcuRsZyc"
      },
      "source": [
        "questions, answers = [],[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzCKSBBYsa_0"
      },
      "source": [
        "# loading train json\n",
        "f = open(\"conala-train.json\",\"r\")\n",
        "train_file = json.load(f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AomiLytOscI-"
      },
      "source": [
        "for num,i in enumerate(train_file):\n",
        "  if i['intent'] is not None:\n",
        "    questions.append(i['intent'])\n",
        "    answers.append(i['snippet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzlNvRtXseTt"
      },
      "source": [
        "# loading train json\n",
        "f = open(\"conala-test.json\",\"r\")\n",
        "test_file = json.load(f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aufa9EAdsgHF"
      },
      "source": [
        "for num,i in enumerate(test_file):\n",
        "  if i['intent'] is not None:\n",
        "    questions.append(i['intent'])\n",
        "    answers.append(i['snippet'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RIPC6d0sgmd"
      },
      "source": [
        "questions_, answers_ = getData(\"session8_assignment.py\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "281koiXzHFOa"
      },
      "source": [
        "questions_, answers_ = getData(\"english_python_data.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9rd6aJhsi28"
      },
      "source": [
        "max_word_len = 301"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk1sHB_QskR3",
        "outputId": "4e92ab2e-0f50-4c99-c034-7024b208c195"
      },
      "source": [
        "# removing examples with len more than max_word_len\n",
        "# Only checking in original data because I checked in CONALA data and answers their dont exceed 301.\n",
        "pruned_questions = []\n",
        "pruned_answers = []\n",
        "for j,i in zip(questions_,answers_):\n",
        "  tokens = getTokenizer(i)\n",
        "  if not len(tokens) > max_word_len:\n",
        "    pruned_answers.append(i)\n",
        "    pruned_questions.append(j)\n",
        "\n",
        "answers_ = pruned_answers\n",
        "questions_ = pruned_questions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQFZJJoPslXn"
      },
      "source": [
        "questions = questions + questions_\n",
        "answers = answers + answers_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7YICE5PsoFe"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhNWcNiSspu2",
        "outputId": "7fdc9260-c13c-4e38-d715-82d875ada9b5"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (54.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myuahwxhsq0D"
      },
      "source": [
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1Qp1svIsrr3"
      },
      "source": [
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kg7kgjJ1ssx2"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "TRG = Field(tokenize = getTokenizer, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = False, \n",
        "            batch_first = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udxaav4asuwX",
        "outputId": "cbe24c03-bf37-4f17-b291-ccc0a7a34652"
      },
      "source": [
        "fields = [('src', SRC), ('trg', TRG)]\n",
        "\n",
        "Examples = [data.Example.fromlist([i,j], fields) for i,j in zip(questions,answers)]\n",
        "Dataset = data.Dataset(Examples, fields)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n",
            "Error in tokenization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knEcb-06sxdN"
      },
      "source": [
        "train_data,valid_data = Dataset.split(split_ratio=[0.90,0.10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cysQisjMsytX"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 1)\n",
        "TRG.build_vocab(train_data, min_freq = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOoJsIFMsz20"
      },
      "source": [
        "# Dumps dicts\n",
        "with open(\"SRC_stio_local\",\"wb\") as f:\n",
        "  pickle.dump(SRC.vocab.stoi,f)\n",
        "with open(\"TRG_itos_local\",\"wb\") as f:\n",
        "  pickle.dump(TRG.vocab.itos,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MGfrdHos1hB"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBxkJOJhs22f"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_key = lambda x: len(x.trg),\n",
        "     device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5ecmWIWs32U"
      },
      "source": [
        "class PositionalEncodingComponent(nn.Module):\n",
        "  '''\n",
        "  Class to encode positional information to tokens.\n",
        "  '''\n",
        "  def __init__(self,hid_dim,device,dropout=0.2,max_len=5000):\n",
        "    super().__init__()\n",
        "\n",
        "    assert hid_dim%2==0 # If not, it will result error in allocation to positional_encodings[:,1::2] later\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.positional_encodings = torch.zeros(max_len,hid_dim)\n",
        "\n",
        "    pos = torch.arange(0,max_len).unsqueeze(1) # pos : [max_len,1]\n",
        "    div_term  = torch.exp(-torch.arange(0,hid_dim,2)*math.log(10000.0)/hid_dim) # Calculating value of 1/(10000^(2i/hid_dim)) in log space and then exponentiating it\n",
        "    # div_term: [hid_dim//2]\n",
        "\n",
        "    self.positional_encodings[:,0::2] = torch.sin(pos*div_term) # pos*div_term [max_len,hid_dim//2]\n",
        "    self.positional_encodings[:,1::2] = torch.cos(pos*div_term) \n",
        "\n",
        "    self.positional_encodings = self.positional_encodings.unsqueeze(0) # To account for batch_size in inputs\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.positional_encodings[:,:x.size(1)].detach().to(self.device)\n",
        "    return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd9ZC5B2s5Jm"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "  def __init__(self, hid_dim, pf_dim, dropout):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "    self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "    x = self.fc_2(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXS2DCmas6cN"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    assert hid_dim % n_heads == 0\n",
        "\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = hid_dim // n_heads\n",
        "\n",
        "    self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "    self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "    self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "    self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  def forward(self, query, key, value, mask = None):\n",
        "\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    Q = self.fc_q(query)\n",
        "    K = self.fc_k(key)\n",
        "    V = self.fc_v(value)\n",
        "\n",
        "    Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "    V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "    energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "    attention = torch.softmax(energy, dim = -1)\n",
        "\n",
        "    x = torch.matmul(self.dropout(attention), V)\n",
        "    x = x.permute(0, 2, 1, 3).contiguous()\n",
        "    x = x.view(batch_size, -1, self.hid_dim)\n",
        "    x = self.fc_o(x)\n",
        "\n",
        "    return x, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebE9eFQvs7pG"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "               hid_dim,\n",
        "               n_heads,\n",
        "               pf_dim,\n",
        "               dropout,\n",
        "               device):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
        "                                                                 pf_dim,\n",
        "                                                                 dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, src, src_mask):\n",
        "\n",
        "    _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "    src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "    _src = self.positionwise_feedforward(src)\n",
        "    src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "    return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf2KWJiLs828"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "               hid_dim,\n",
        "               n_heads,\n",
        "               pf_dim,\n",
        "               dropout,\n",
        "               device):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "    self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "    self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
        "                                                                 pf_dim,\n",
        "                                                                 dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "  \n",
        "  def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "    _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "    trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "    trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    _trg = self.positionwise_feedforward(trg)\n",
        "    trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "    return trg, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTMLucuDs-GH"
      },
      "source": [
        "# class Encoder(nn.Module):\n",
        "#   def __init__(self,\n",
        "#                input_dim,\n",
        "#                hid_dim,\n",
        "#                n_layers,\n",
        "#                n_heads,\n",
        "#                pf_dim,\n",
        "#                dropout,\n",
        "#                device,\n",
        "#                max_length=100):\n",
        "\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.device = device\n",
        "\n",
        "#     self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "#     self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "\n",
        "#     self.layers = nn.ModuleList([EncoderLayer(hid_dim,\n",
        "#                                               n_heads,\n",
        "#                                               pf_dim,\n",
        "#                                               dropout,\n",
        "#                                               device)\n",
        "#                                 for _ in range(n_layers)])\n",
        "    \n",
        "#     self.dropout = nn.Dropout(dropout)\n",
        "#     self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "#   def forward(self, src, src_mask):\n",
        "\n",
        "#     batch_size = src.shape[0]\n",
        "#     src_len = src.shape[1]\n",
        "\n",
        "#     pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "#     src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "#     for layer in self.layers:\n",
        "#       src = layer(src, src_mask)\n",
        "\n",
        "#     return src  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lHNRNd-0c4v"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length = 5000):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(input_dim,hid_dim)\n",
        "    self.pos_embedding = PositionalEncodingComponent(hid_dim,device,dropout,max_length)\n",
        "\n",
        "    self.layers = nn.ModuleList([EncoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "  def forward(self,src,src_mask):\n",
        "\n",
        "    batch_size = src.shape[0]\n",
        "    src_len = src.shape[1]\n",
        "\n",
        "    tok_embeddings = self.tok_embedding(src)*self.scale\n",
        "    src  = self.pos_embedding(tok_embeddings)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      src = layer(src,src_mask)\n",
        "\n",
        "    return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8miASDIcs_W4"
      },
      "source": [
        "# class Decoder(nn.Module):\n",
        "#   def __init__(self,\n",
        "#                output_dim,\n",
        "#                hid_dim,\n",
        "#                n_layers,\n",
        "#                n_heads,\n",
        "#                pf_dim,\n",
        "#                dropout,\n",
        "#                device,\n",
        "#                max_length=100):\n",
        "\n",
        "#     super().__init__()\n",
        "\n",
        "#     self.device = device\n",
        "\n",
        "#     self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "#     self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "\n",
        "#     self.layers = nn.ModuleList([DecoderLayer(hid_dim,\n",
        "#                                               n_heads,\n",
        "#                                               pf_dim,\n",
        "#                                               dropout,\n",
        "#                                               device)\n",
        "#                                 for _ in range(n_layers)])\n",
        "    \n",
        "#     self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "#     self.dropout = nn.Dropout(dropout)\n",
        "#     self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "#   def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "#     batch_size = trg.shape[0]\n",
        "#     trg_len = trg.shape[1]\n",
        "\n",
        "#     pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "#     trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "#     for layer in self.layers:\n",
        "#       trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "#     output = self.fc_out(trg)\n",
        "\n",
        "#     return output, attention "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGVcafvi0lSZ"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,output_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length= 5000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.tok_embedding = nn.Embedding(output_dim,hid_dim)\n",
        "    self.pos_embedding = PositionalEncodingComponent(hid_dim,device,dropout,max_length)\n",
        "\n",
        "    self.layers = nn.ModuleList([DecoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
        "\n",
        "    self.fc_out = nn.Linear(hid_dim,output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "  def forward(self, trg, enc_src,trg_mask,src_mask):\n",
        "    \n",
        "    batch_size = trg.shape[0]\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    tok_embeddings = self.tok_embedding(trg)*self.scale\n",
        "\n",
        "    trg = self.pos_embedding(tok_embeddings)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      trg, encoder_attention = layer(trg,enc_src,trg_mask,src_mask)\n",
        "    \n",
        "    output = self.fc_out(trg)\n",
        "    \n",
        "    return output, encoder_attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h5WCU6QtMGY"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self,\n",
        "               encoder,\n",
        "               decoder,\n",
        "               src_pad_idx,\n",
        "               trg_pad_idx,\n",
        "               device):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self, src):\n",
        "\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    return src_mask\n",
        "\n",
        "  def make_trg_mask(self, trg):\n",
        "\n",
        "    trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    trg_len = trg.shape[1]\n",
        "\n",
        "    trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "\n",
        "    trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "    return trg_mask\n",
        "\n",
        "  def forward(self, src, trg):\n",
        "\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "    enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "    output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "    return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzpNoVsNtNWh"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ1yBn-etOkx"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn-Dyn9VtR0Y"
      },
      "source": [
        "# load python embedding weights\n",
        "pretrained_embeddings = torch.load('python_embedding_weigts.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P8WLNkctS38"
      },
      "source": [
        "# Loading pretrained embeddings\n",
        "with open(\"TRG_stio\",\"rb\") as f:\n",
        "  stoi_weights = pickle.load(f)\n",
        "\n",
        "with torch.no_grad():\n",
        "  indexes = []\n",
        "  for i,j in enumerate(TRG.vocab.stoi):\n",
        "    if j in stoi_weights:\n",
        "      model.decoder.tok_embedding.weight[TRG.vocab.stoi[j]] = pretrained_embeddings.weight[stoi_weights[j]]\n",
        "    else:\n",
        "      model.decoder.tok_embedding.weight[TRG.vocab.stoi[j]] = pretrained_embeddings.weight[stoi_weights['<unk>']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrm-WuuItVF6",
        "outputId": "779513bd-9eb0-4215-accc-2cd1a22fceec"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 7,140,960 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHBV7qxVtWG_"
      },
      "source": [
        "LEARNING_RATE = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "# criterion = nn.KLDivLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy-kE1PItejz"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "                \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Sa1ELXtf2q"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32X8eYwWthRa"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_TMmpwVtidN",
        "outputId": "cd192b07-1f49-46a9-9a35-ded5ca23361d"
      },
      "source": [
        "N_EPOCHS = 50\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "best_train_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'conala_plus_original_data.pt')\n",
        "    \n",
        "    if train_loss < best_train_loss:\n",
        "        best_train_loss = train_loss\n",
        "        torch.save(model.state_dict(), 'best_train_loss.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 18s\n",
            "\tTrain Loss: 5.567 | Train PPL: 261.598\n",
            "\t Val. Loss: 3.930 |  Val. PPL:  50.903\n",
            "Epoch: 02 | Time: 0m 17s\n",
            "\tTrain Loss: 3.636 | Train PPL:  37.933\n",
            "\t Val. Loss: 3.233 |  Val. PPL:  25.349\n",
            "Epoch: 03 | Time: 0m 17s\n",
            "\tTrain Loss: 3.190 | Train PPL:  24.300\n",
            "\t Val. Loss: 2.962 |  Val. PPL:  19.341\n",
            "Epoch: 04 | Time: 0m 17s\n",
            "\tTrain Loss: 2.962 | Train PPL:  19.345\n",
            "\t Val. Loss: 2.813 |  Val. PPL:  16.659\n",
            "Epoch: 05 | Time: 0m 17s\n",
            "\tTrain Loss: 2.801 | Train PPL:  16.465\n",
            "\t Val. Loss: 2.676 |  Val. PPL:  14.531\n",
            "Epoch: 06 | Time: 0m 17s\n",
            "\tTrain Loss: 2.672 | Train PPL:  14.470\n",
            "\t Val. Loss: 2.587 |  Val. PPL:  13.290\n",
            "Epoch: 07 | Time: 0m 17s\n",
            "\tTrain Loss: 2.567 | Train PPL:  13.024\n",
            "\t Val. Loss: 2.504 |  Val. PPL:  12.236\n",
            "Epoch: 08 | Time: 0m 17s\n",
            "\tTrain Loss: 2.474 | Train PPL:  11.865\n",
            "\t Val. Loss: 2.444 |  Val. PPL:  11.515\n",
            "Epoch: 09 | Time: 0m 17s\n",
            "\tTrain Loss: 2.395 | Train PPL:  10.964\n",
            "\t Val. Loss: 2.394 |  Val. PPL:  10.962\n",
            "Epoch: 10 | Time: 0m 17s\n",
            "\tTrain Loss: 2.321 | Train PPL:  10.182\n",
            "\t Val. Loss: 2.349 |  Val. PPL:  10.476\n",
            "Epoch: 11 | Time: 0m 17s\n",
            "\tTrain Loss: 2.256 | Train PPL:   9.542\n",
            "\t Val. Loss: 2.309 |  Val. PPL:  10.061\n",
            "Epoch: 12 | Time: 0m 17s\n",
            "\tTrain Loss: 2.195 | Train PPL:   8.981\n",
            "\t Val. Loss: 2.272 |  Val. PPL:   9.696\n",
            "Epoch: 13 | Time: 0m 17s\n",
            "\tTrain Loss: 2.139 | Train PPL:   8.487\n",
            "\t Val. Loss: 2.246 |  Val. PPL:   9.451\n",
            "Epoch: 14 | Time: 0m 17s\n",
            "\tTrain Loss: 2.090 | Train PPL:   8.087\n",
            "\t Val. Loss: 2.214 |  Val. PPL:   9.151\n",
            "Epoch: 15 | Time: 0m 17s\n",
            "\tTrain Loss: 2.037 | Train PPL:   7.668\n",
            "\t Val. Loss: 2.192 |  Val. PPL:   8.955\n",
            "Epoch: 16 | Time: 0m 17s\n",
            "\tTrain Loss: 1.991 | Train PPL:   7.321\n",
            "\t Val. Loss: 2.174 |  Val. PPL:   8.797\n",
            "Epoch: 17 | Time: 0m 17s\n",
            "\tTrain Loss: 1.946 | Train PPL:   7.004\n",
            "\t Val. Loss: 2.158 |  Val. PPL:   8.650\n",
            "Epoch: 18 | Time: 0m 17s\n",
            "\tTrain Loss: 1.901 | Train PPL:   6.690\n",
            "\t Val. Loss: 2.134 |  Val. PPL:   8.451\n",
            "Epoch: 19 | Time: 0m 17s\n",
            "\tTrain Loss: 1.861 | Train PPL:   6.429\n",
            "\t Val. Loss: 2.121 |  Val. PPL:   8.337\n",
            "Epoch: 20 | Time: 0m 17s\n",
            "\tTrain Loss: 1.820 | Train PPL:   6.169\n",
            "\t Val. Loss: 2.108 |  Val. PPL:   8.235\n",
            "Epoch: 21 | Time: 0m 17s\n",
            "\tTrain Loss: 1.783 | Train PPL:   5.949\n",
            "\t Val. Loss: 2.093 |  Val. PPL:   8.108\n",
            "Epoch: 22 | Time: 0m 17s\n",
            "\tTrain Loss: 1.746 | Train PPL:   5.732\n",
            "\t Val. Loss: 2.080 |  Val. PPL:   8.004\n",
            "Epoch: 23 | Time: 0m 17s\n",
            "\tTrain Loss: 1.712 | Train PPL:   5.542\n",
            "\t Val. Loss: 2.066 |  Val. PPL:   7.892\n",
            "Epoch: 24 | Time: 0m 17s\n",
            "\tTrain Loss: 1.679 | Train PPL:   5.361\n",
            "\t Val. Loss: 2.064 |  Val. PPL:   7.879\n",
            "Epoch: 25 | Time: 0m 17s\n",
            "\tTrain Loss: 1.644 | Train PPL:   5.178\n",
            "\t Val. Loss: 2.056 |  Val. PPL:   7.818\n",
            "Epoch: 26 | Time: 0m 17s\n",
            "\tTrain Loss: 1.611 | Train PPL:   5.007\n",
            "\t Val. Loss: 2.044 |  Val. PPL:   7.724\n",
            "Epoch: 27 | Time: 0m 17s\n",
            "\tTrain Loss: 1.584 | Train PPL:   4.876\n",
            "\t Val. Loss: 2.030 |  Val. PPL:   7.617\n",
            "Epoch: 28 | Time: 0m 17s\n",
            "\tTrain Loss: 1.550 | Train PPL:   4.714\n",
            "\t Val. Loss: 2.037 |  Val. PPL:   7.671\n",
            "Epoch: 29 | Time: 0m 17s\n",
            "\tTrain Loss: 1.523 | Train PPL:   4.584\n",
            "\t Val. Loss: 2.034 |  Val. PPL:   7.646\n",
            "Epoch: 30 | Time: 0m 17s\n",
            "\tTrain Loss: 1.494 | Train PPL:   4.454\n",
            "\t Val. Loss: 2.022 |  Val. PPL:   7.550\n",
            "Epoch: 31 | Time: 0m 17s\n",
            "\tTrain Loss: 1.469 | Train PPL:   4.346\n",
            "\t Val. Loss: 2.020 |  Val. PPL:   7.538\n",
            "Epoch: 32 | Time: 0m 17s\n",
            "\tTrain Loss: 1.439 | Train PPL:   4.218\n",
            "\t Val. Loss: 2.020 |  Val. PPL:   7.539\n",
            "Epoch: 33 | Time: 0m 17s\n",
            "\tTrain Loss: 1.415 | Train PPL:   4.115\n",
            "\t Val. Loss: 2.007 |  Val. PPL:   7.442\n",
            "Epoch: 34 | Time: 0m 17s\n",
            "\tTrain Loss: 1.393 | Train PPL:   4.025\n",
            "\t Val. Loss: 2.008 |  Val. PPL:   7.446\n",
            "Epoch: 35 | Time: 0m 17s\n",
            "\tTrain Loss: 1.365 | Train PPL:   3.914\n",
            "\t Val. Loss: 2.021 |  Val. PPL:   7.545\n",
            "Epoch: 36 | Time: 0m 17s\n",
            "\tTrain Loss: 1.342 | Train PPL:   3.826\n",
            "\t Val. Loss: 2.008 |  Val. PPL:   7.446\n",
            "Epoch: 37 | Time: 0m 17s\n",
            "\tTrain Loss: 1.318 | Train PPL:   3.734\n",
            "\t Val. Loss: 2.019 |  Val. PPL:   7.533\n",
            "Epoch: 38 | Time: 0m 17s\n",
            "\tTrain Loss: 1.291 | Train PPL:   3.637\n",
            "\t Val. Loss: 2.010 |  Val. PPL:   7.462\n",
            "Epoch: 39 | Time: 0m 17s\n",
            "\tTrain Loss: 1.273 | Train PPL:   3.571\n",
            "\t Val. Loss: 2.007 |  Val. PPL:   7.444\n",
            "Epoch: 40 | Time: 0m 17s\n",
            "\tTrain Loss: 1.249 | Train PPL:   3.488\n",
            "\t Val. Loss: 2.018 |  Val. PPL:   7.527\n",
            "Epoch: 41 | Time: 0m 17s\n",
            "\tTrain Loss: 1.229 | Train PPL:   3.419\n",
            "\t Val. Loss: 2.013 |  Val. PPL:   7.486\n",
            "Epoch: 42 | Time: 0m 17s\n",
            "\tTrain Loss: 1.206 | Train PPL:   3.339\n",
            "\t Val. Loss: 2.010 |  Val. PPL:   7.465\n",
            "Epoch: 43 | Time: 0m 17s\n",
            "\tTrain Loss: 1.186 | Train PPL:   3.273\n",
            "\t Val. Loss: 2.016 |  Val. PPL:   7.509\n",
            "Epoch: 44 | Time: 0m 17s\n",
            "\tTrain Loss: 1.164 | Train PPL:   3.203\n",
            "\t Val. Loss: 2.021 |  Val. PPL:   7.544\n",
            "Epoch: 45 | Time: 0m 17s\n",
            "\tTrain Loss: 1.148 | Train PPL:   3.151\n",
            "\t Val. Loss: 2.008 |  Val. PPL:   7.449\n",
            "Epoch: 46 | Time: 0m 17s\n",
            "\tTrain Loss: 1.131 | Train PPL:   3.098\n",
            "\t Val. Loss: 2.021 |  Val. PPL:   7.543\n",
            "Epoch: 47 | Time: 0m 17s\n",
            "\tTrain Loss: 1.112 | Train PPL:   3.042\n",
            "\t Val. Loss: 2.019 |  Val. PPL:   7.529\n",
            "Epoch: 48 | Time: 0m 17s\n",
            "\tTrain Loss: 1.091 | Train PPL:   2.976\n",
            "\t Val. Loss: 2.023 |  Val. PPL:   7.559\n",
            "Epoch: 49 | Time: 0m 17s\n",
            "\tTrain Loss: 1.075 | Train PPL:   2.929\n",
            "\t Val. Loss: 2.027 |  Val. PPL:   7.588\n",
            "Epoch: 50 | Time: 0m 17s\n",
            "\tTrain Loss: 1.059 | Train PPL:   2.883\n",
            "\t Val. Loss: 2.037 |  Val. PPL:   7.668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0GzsP06tjwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4006620f-3cca-4ea8-8998-5140f95e1745"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"SRC_stio_local\",\"rb\") as f:\n",
        "  stoi = pickle.load(f)\n",
        "with open(\"TRG_itos_local\",\"rb\") as f:\n",
        "  itos = pickle.load(f)\n",
        "\n",
        "# Load model\n",
        "# trained_model = 'conala_plus_original_data.pt'\n",
        "trained_model = 'best_train_loss.pt'\n",
        "model.load_state_dict(torch.load(trained_model));\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(3024, 256)\n",
              "    (pos_embedding): PositionalEncodingComponent(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(4704, 256)\n",
              "    (pos_embedding): PositionalEncodingComponent(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=4704, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trppsFEEtlCJ"
      },
      "source": [
        "import spacy\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def encode_inputs(input,vocab):\n",
        "\n",
        "  tokenized_input_ = [tok.text.lower() for tok in spacy_en.tokenizer(input)]\n",
        "  tokenized_input = ['<sos>'] + tokenized_input_ +['<eos>']\n",
        "\n",
        "  numericalized_input = [vocab[i] for i in tokenized_input]\n",
        "\n",
        "  tensor_input = torch.LongTensor([numericalized_input])\n",
        "  \n",
        "  return tensor_input,tokenized_input_\n",
        "\n",
        "def decode_outputs(output,vocab):\n",
        "  # output: [1,1,hid_dim]\n",
        "  predicted_token = output.argmax(-1)\n",
        "  return vocab[predicted_token.item()], predicted_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOSlXnRXtmks"
      },
      "source": [
        "def display_attention(sentence, translation, attention, n_heads = 8, n_rows = 4, n_cols = 2):\n",
        "    \n",
        "    assert n_rows * n_cols == n_heads\n",
        "    \n",
        "    fig = plt.figure(figsize=(15,25))\n",
        "    \n",
        "    for i in range(n_heads):\n",
        "        \n",
        "        ax = fig.add_subplot(n_rows, n_cols, i+1)\n",
        "        \n",
        "        _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
        "\n",
        "        cax = ax.matshow(_attention, cmap='bone')\n",
        "\n",
        "        ax.tick_params(labelsize=12)\n",
        "        ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                           rotation=45)\n",
        "        ax.set_yticklabels(['']+translation)\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez0Ysvp1tn_8"
      },
      "source": [
        "def print_decoder_output(decoder_outputs):\n",
        "  # print(decoder_outputs)\n",
        "  combined_output = \" \".join(decoder_outputs)\n",
        "  pruned_output = re.sub(r'\\n |\\n  ',r'\\n',combined_output)\n",
        "  # pruned_output = re.sub(\" . \",\".\",pruned_output)\n",
        "  # pruned_output = combined_output\n",
        "  print(pruned_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InMKBqSZsNf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d937f8f-c15f-4009-ee0f-2b5f8bfd5736"
      },
      "source": [
        " print(\" Enter q or quit to exit.\")\n",
        "\n",
        "answer_max_len = 200\n",
        "\n",
        "while(True):\n",
        "\n",
        "  input_ = input(\"Enter text:\")\n",
        "\n",
        "  if input_=='q' or input_=='quit':\n",
        "    break\n",
        "\n",
        "  src,tokenized_input_ = encode_inputs(input_,stoi)\n",
        "  src = src.to(device)\n",
        "  # src_mask = torch.ones([1,1,1,src.shape[-1]]).to(device)\n",
        "  src_mask = model.make_src_mask(src)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    enc_src = model.encoder(src,src_mask)\n",
        "  \n",
        "  trg = '<sos>'\n",
        "  trg_indexes = [stoi[trg]]\n",
        "  # trg_mask = torch.ones([1,1,1,1]).to(device)\n",
        "\n",
        "  decoder_outputs = []\n",
        "  for i in range(answer_max_len):\n",
        "    trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "    trg_mask = model.make_trg_mask(trg_tensor)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      decoder_output,encoder_decoder_attention = model.decoder(trg_tensor,enc_src,trg_mask,src_mask)\n",
        "\n",
        "    pred_token = decoder_output.argmax(2)[:,-1].item()\n",
        "\n",
        "    if pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
        "      break\n",
        "    decoder_outputs.append(itos[pred_token])\n",
        "    trg_indexes.append(pred_token)\n",
        "\n",
        "\n",
        "  print_decoder_output(decoder_outputs)\n",
        "  # display_attention(tokenized_input_,decoder_outputs,encoder_decoder_attention,DEC_HEADS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Enter q or quit to exit.\n",
            "Enter text:program to calculate simple interest\n",
            "def simple_interest ( p , r ) : \n",
            "\t si = ( p * t * r ) / 100 \n",
            "return si \n",
            "Enter text:function to sum odd elements of list\n",
            "sum ( sum ( i for i in range ( len ( l ) ) ) \n",
            "Enter text:reverse a string\n",
            "' ' . join ( reversed ( reversed ( reversed ( reversed ( reversed ( reversed ( reversed ( s ) ) ) ) ) ) ) \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}