EVA5 Capstone Project
=====================

This project is to to create a network that can perform 3 tasks simultaneously:

1. Predict the boots, PPE, hardhat, and mask if there is an image.
2. Predict the depth map of the image.
3. Predict the Planar Surfaces in the region.

Data preparation
================

Input images were collected from all the batch students who individually contributed between 50-200 images (at least 50 images of each class). Almost all of them are .jpg/.jpeg files. Bounding Boxes Bounding boxes are created using annotation tool from here: https://github.com/miki998/YoloV3_Annotation_Tool. Total images collected were 3591. Another 144 images were individually collected and annotated using the tool. In addition, high quality images were captured from a short video of construction workers dancing and occupied the frame during the entire duration. All the necessary data was initially loaded to google drive and then downloaded from there.

Model
=====

The model uses an Encoder-Decoder architecture.

Common encoder:

Used the pre-trained **resnext101_32x8d_wsl** for encoding with pre-trained weights. It is maintained by Facebook and has better performance than ResNet101 (Darknet encoder is closely couple with Yolo decoder).

We have 3 decoders:

1. YoloV3 for object detection (https://github.com/theschoolofai/YoloV3) - Used almost as-is except for the encoding section. Darknet-53 used for feature extraction (up to the 75th layer). There are 3 skip connections in the existing code from the Darknet encoder to decoder. Used a 1x1 to connect our encoder output to yolo decoder.
2. MiDaS for depth map detection (https://github.com/intel-isl/MiDaS) - Again, used almost as-is except for the encoding section.
3. Planercnn for surface plane images (https://github.com/NVlabs/planercnn). - Retained most of the original code. It is a MaskRCNN network consisting of ResNet101 for feature extraction followed by FPN, RPN and remaining layers for detection. Changed FPN layers to connect to our encoder (instead of the default one).

**Note:** Since nms and roi of planercnn integration (from Session 14) were using pytorch 0.4, it had to be replaced with torchvision nms and roi from https://github.com/longcw/RoIAlign.pytorch to be compatible with others.

Architecture
============


Loss functions
==============

1. YoloV3 loss function - It combines a Sigmoid layer and BCELoss in one layer (No Softmax) instead of the former followed by the latter as it is numerically more stable.
2. MiDas - Uses 2 separate loss functions and sums them up at the end. First is RMSE (Root Mean Square Error) and second is SSIM (Structural Similarity Index Measure) which finds the similarity between the pixels of midas ground truth and the images of inverse depth maps generated by the model. Loss = (1 - SSIM Index).
3. Planercnn - Uses 3 separate loss functions and sums them up at the end. First one uses cross_entropy loss to compare rpn_class and rpn_bbox, second uses MSE (Mean Squared Error) to compare the plane_parameters.npy & plane_masks.npy with the predicted np arrays and third uses SSIM to compare the predicted segmentation image with the target.

Overall loss is the sum of the above 3 (Giving equal weightage to each).

Optimizer
=========

Standard SGD (Stochastic Gradient Descent) used with the already set paramter values. Scheduler is LambdaLR.

Training
========

The training process is copied from Yolo code base (MiDaS code base training was used for inference). Added arguments for Yolo and MiDaS to the options file from planercnn detection (Separate function for each of the 3 tasks). Trained the model for 10 epochs (https://github.com/paishowstopper/TSAI/blob/main/EVA5/Capstone/EVA5_Capstone_Round_1.ipynb and https://github.com/paishowstopper/TSAI/blob/main/EVA5/Capstone/EVA5_Capstone_Round_2.ipynb). Each epoch took an average of 55 minutes to complete. The overall loss reduced during the initial epochs but started increasing during the final epochs. MiDaS and Planercnn losses were fairly consistent during the entire training but Yolo loss was fluctuating. 

**Minimum loss was achieved midway: 9.91.**
**Best model:** https://drive.google.com/file/d/1-1ETrvgU0LFVTz7GU7iRNX9nTgLqLDU0/view?usp=sharing

Best model was used for evaluation to generate the final output. Sample output shown here: https://github.com/paishowstopper/TSAI/tree/main/EVA5/Capstone/output_images

Additional points
=================
